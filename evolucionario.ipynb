{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weat 6\n",
    "_glove_model = gensim.downloader.load('glove-wiki-gigaword-50')\n",
    "_glove_model_changed = gensim.downloader.load('glove-wiki-gigaword-50')\n",
    "\n",
    "#careers = ['technician', 'accountant', 'supervisor', 'engineer', 'worker', 'educator', 'clerk', 'counselor', 'inspector', 'mechanic', 'manager', 'therapist', 'administrator', 'salesperson', 'receptionist', 'librarian', 'advisor', 'pharmacist', 'janitor', 'psychologist', 'physician', 'carpenter', 'nurse', 'investigator', 'bartender', 'specialist', 'electrician', 'officer', 'pathologist', 'teacher', 'lawyer', 'planner', 'practitioner', 'plumber', 'instructor', 'surgeon', 'veterinarian', 'paramedic', 'examiner', 'chemist', 'machinist', 'appraiser', 'nutritionist', 'architect', 'hairdresser', 'baker', 'programmer', 'paralegal', 'hygienist', 'scientist']\n",
    "female_attributes = ['female', 'woman', 'girl', 'sister', 'she', 'her', 'hers', 'daughter']\n",
    "male_attributes = ['male', 'man', 'boy', 'brother', 'he', 'him', 'his', 'son']\n",
    "\n",
    "careers = ['math', 'algebra', 'geometry', 'calculus', 'equations', 'computation', 'numbers', 'addition', 'poetry', 'art', 'dance', 'literature', 'novel', 'symphony', 'drama', 'sculpture']\n",
    "\n",
    "all_words = list(set(female_attributes + male_attributes + careers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = {word: _glove_model[word] for word in all_words if word in _glove_model}\n",
    "vectors = np.array([word_vectors[word] for word in word_vectors.keys()])\n",
    "words = list(word_vectors.keys())\n",
    "reduced_glove = Word2Vec(vector_size=50, min_count=1)\n",
    "reduced_glove.build_vocab([words])\n",
    "reduced_glove.wv.vectors = vectors\n",
    "reduced_glove.wv.index_to_key = words\n",
    "reduced_glove.wv.key_to_index = {word: idx for idx, word in enumerate(words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "POP_SIZE = 50\n",
    "GEN_MAX = 100\n",
    "MUT_RATE = 0.2\n",
    "CROSS_RATE = 0.6\n",
    "\n",
    "def noise_vec(_size=50):\n",
    "    return np.random.uniform(-2, 2, size=_size)\n",
    "\n",
    "def init_population():\n",
    "    return np.ones((POP_SIZE, 50)) + np.array([noise_vec() for _ in range(POP_SIZE)])\n",
    "\n",
    "def selection(population, costs, _k=2):\n",
    "    return random.choices(population, weights=[1 / (cost + 1) for cost in costs], k=_k)\n",
    "\n",
    "def crossover(parent1, parent2):\n",
    "    point1, point2 = sorted(random.sample(range(len(parent1)), 2))\n",
    "    child1 = np.concatenate((parent1[:point1], parent2[point1:point2], parent1[point2:]))\n",
    "    child2 = np.concatenate((parent2[:point1], parent1[point1:point2], parent2[point2:]))\n",
    "    return child1, child2\n",
    "\n",
    "def mutate(child):\n",
    "    if random.random() < MUT_RATE:\n",
    "        child += noise_vec()\n",
    "    return child\n",
    "\n",
    "def load_dataset(filepath):\n",
    "    word_pairs = []\n",
    "    human_scores = []\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 3:\n",
    "                word1, word2, score = parts\n",
    "                score = float(score)\n",
    "                word_pairs.append((word1.split('-')[0], word2.split('-')[0]))\n",
    "                human_scores.append(score)\n",
    "    return word_pairs, human_scores\n",
    "\n",
    "def wordembedding_similarity(word_pairs, model):\n",
    "    embedding_similarities = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        if word1 in model and word2 in model:\n",
    "            similarity = model.similarity(word1, word2)\n",
    "        else:\n",
    "            similarity = 0.0\n",
    "        embedding_similarities.append(similarity)\n",
    "    return embedding_similarities\n",
    "\n",
    "def combined_cost(s, word_pairs, human_scores, alpha=0.5):\n",
    "    #glove_model_modified = _glove_model\n",
    "    glove_model_modified = copy.deepcopy(reduced_glove.wv)\n",
    "    \n",
    "    for word in all_words:\n",
    "        if word in glove_model_modified:\n",
    "            glove_model_modified[word] = np.multiply(glove_model_modified[word], s)\n",
    "\n",
    "\n",
    "    differences = [\n",
    "        abs(\n",
    "            sum(glove_model_modified.similarity(career, attr) for attr in female_attributes) / len(female_attributes) - \n",
    "            sum(glove_model_modified.similarity(career, attr) for attr in male_attributes) / len(male_attributes)\n",
    "        )\n",
    "        for career in careers\n",
    "    ]\n",
    "    \n",
    "    bias = sum(differences)\n",
    "    embedding_similarities = wordembedding_similarity(word_pairs, glove_model_modified)\n",
    "    spearman_corr, _ = spearmanr(human_scores, embedding_similarities)\n",
    "    cost_spearman = 1 - spearman_corr\n",
    "    \n",
    "    return alpha * bias + (1 - alpha) * cost_spearman\n",
    "\n",
    "def evolutionary_algorithm(word_pairs, human_scores, alpha=0.5):\n",
    "    population = init_population()\n",
    "    best_individual = None\n",
    "    best_cost = float('inf')\n",
    "    \n",
    "    for gen in range(GEN_MAX):\n",
    "        costs = [combined_cost(individual, word_pairs, human_scores, alpha) for individual in population]\n",
    "        best_idx = np.argmin(costs)\n",
    "        current_best_cost = costs[best_idx]\n",
    "        current_best_individual = population[best_idx]\n",
    "        \n",
    "        if current_best_cost < best_cost:\n",
    "            best_cost = current_best_cost\n",
    "            best_individual = current_best_individual\n",
    "        \n",
    "        new_population = [best_individual]\n",
    "        if gen % 10 == 0:\n",
    "            print(f\"Generation {gen}/{GEN_MAX}: Best Cost {best_cost}\")\n",
    "        \n",
    "        while len(new_population) < POP_SIZE:\n",
    "            parent1, parent2 = selection(population, costs)\n",
    "            if random.random() < CROSS_RATE:\n",
    "                child1, child2 = crossover(parent1, parent2)\n",
    "                child1 = mutate(child1)\n",
    "                child2 = mutate(child2)\n",
    "                new_population.extend([child1, child2])\n",
    "            else:\n",
    "                new_population.extend([parent1, parent2])\n",
    "        \n",
    "        population = new_population[:POP_SIZE]\n",
    "    \n",
    "    print(f\"Generation {100}/{GEN_MAX}: Best Cost {best_cost}\")\n",
    "    return best_individual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'MEN_dataset_lemma_form.dev'\n",
    "word_pairs, human_scores = load_dataset(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliação inicial:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8004963350659737"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Avaliação inicial:\")\n",
    "combined_cost(np.ones(50), word_pairs, human_scores, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 0/100: Best Cost 0.7545248057405034\n",
      "Generation 10/100: Best Cost 0.6911004585391411\n",
      "Generation 20/100: Best Cost 0.6547464189762751\n",
      "Generation 30/100: Best Cost 0.6407461542813608\n",
      "Generation 40/100: Best Cost 0.6407461542813608\n",
      "Generation 50/100: Best Cost 0.6081665392676869\n",
      "Generation 60/100: Best Cost 0.6081665392676869\n",
      "Generation 70/100: Best Cost 0.5899446915546933\n",
      "Generation 80/100: Best Cost 0.5834471133249381\n",
      "Generation 90/100: Best Cost 0.5822042028131464\n",
      "Generation 100/100: Best Cost 0.5757988010156175\n"
     ]
    }
   ],
   "source": [
    "best_vector = evolutionary_algorithm(word_pairs, human_scores, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avaliação final:\n",
      "0.5757988010156175\n"
     ]
    }
   ],
   "source": [
    "print(\"Avaliação final:\")\n",
    "print(combined_cost(best_vector, word_pairs, human_scores, alpha=0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gender_bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
